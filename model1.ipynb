{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08aec0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20ca0fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size= 256\n",
    "batch_size= 32\n",
    "channels= 3\n",
    "epochs= 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e4535",
   "metadata": {},
   "source": [
    "Constants have been set successfully!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53934360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Potato___Early_Blight', 'Potato___Healthy', 'Potato___Late_Blight']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path=\"potato2\"\n",
    "class_names= os.listdir(dir_path)\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cd82d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Potato___Early_Blight', 'Potato___Healthy',\n",
       "       'Potato___Late_Blight'], dtype='<U21')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le= LabelEncoder() #holds the label for each class\n",
    "le.fit(class_names) #class names have been fit into an array\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d9cf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=[],[]\n",
    "\n",
    "for c in class_names:\n",
    "    class_dir= os.path.join(dir_path,c)\n",
    "    for f in os.listdir(class_dir):\n",
    "        img= Image.open(os.path.join(class_dir,f))\n",
    "        img= img.resize((img_size,img_size))\n",
    "        img= np.array(img)/ 255.0 #normalize pixel value (8-bit image)\n",
    "        x.append(img)\n",
    "        y.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b03007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.array(x)\n",
    "y= le.transform(y) #transforms the class names into labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d638649",
   "metadata": {},
   "source": [
    "Split the dataset into Training, Validation and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96bd117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_temp,y_train,y_temp= train_test_split(x, y, test_size=0.2, random_state=123)\n",
    "x_val,x_test,y_val,y_test= train_test_split(x_temp,y_temp, test_size=0.5, random_state=123)\n",
    "#80% data for Training\n",
    "#10% data for validation\n",
    "#10% data for ttesting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3868b639",
   "metadata": {},
   "source": [
    "Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ce7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(x_train[i])\n",
    "    plt.title(class_names[y_train[i]])\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfedd77",
   "metadata": {},
   "source": [
    "Shuffle and Cache the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "114850ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the training dataset\n",
    "shuffle_ind= np.arange(len(x_train))\n",
    "np.random.shuffle(shuffle_ind)\n",
    "x_train= x_train[shuffle_ind]\n",
    "y_train= y_train[shuffle_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "052352d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the validation dataset\n",
    "shuffle_ind= np.arange(len(x_val))\n",
    "np.random.shuffle(shuffle_ind)\n",
    "x_val= x_val[shuffle_ind]\n",
    "y_val= y_val[shuffle_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e8bc015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the test dataset\n",
    "shuffle_ind= np.arange(len(x_test))\n",
    "np.random.shuffle(shuffle_ind)\n",
    "x_test= x_test[shuffle_ind]\n",
    "y_test= y_test[shuffle_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f5935b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating data generators to iterate over the data in batches (Prefetching)\n",
    "def data_generator(x,y,batch_size):\n",
    "    samples=len(x)\n",
    "    while True:\n",
    "        for i in range(0,samples,batch_size):\n",
    "            x_batch= x[i:i + batch_size]\n",
    "            y_batch= y[i:i + batch_size]\n",
    "            yield x_batch,y_batch\n",
    "\n",
    "            \n",
    "#iterating over the data using generators\n",
    "train_dg= data_generator(x_train,y_train,batch_size)\n",
    "val_dg= data_generator(x_val,y_val,batch_size)\n",
    "test_dg= data_generator(x_test,y_test,batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63eb45",
   "metadata": {},
   "source": [
    "Model Architecture (CNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "41d51e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3380829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class convLayer:\n",
    "    def __init__(self, filters, kernels, channels, activation):\n",
    "        self.filters = filters\n",
    "        self.kernels = kernels\n",
    "        self.activation = activation\n",
    "        self.weights = np.random.randn(kernels, kernels, channels, filters) * 0.01\n",
    "        self.bias = np.zeros((filters,))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch_size, input_height, input_width, input_channels = inputs.shape\n",
    "        output_height = input_height - self.kernels + 1\n",
    "        output_width = input_width - self.kernels + 1\n",
    "        \n",
    "        self.output = np.zeros((batch_size, output_height, output_width, self.filters))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for a in range(output_height):\n",
    "                for b in range(output_width):\n",
    "                    for j in range(self.filters):\n",
    "                        # Extract the corresponding input slice\n",
    "                        input_slice = inputs[i, a:a + self.kernels, b:b + self.kernels, :]\n",
    "                        # Perform element-wise multiplication and sum\n",
    "                        self.output[i, a, b, j] = np.sum(input_slice * self.weights[:, :, :, j]) + self.bias[j]\n",
    "            \n",
    "        self.output = self.activation(self.output)\n",
    "        return self.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "63998a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class maxpoolLayer:\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch_size, input_height, input_width, input_channels = inputs.shape\n",
    "        output_height = input_height // self.pool_size\n",
    "        output_width = input_width // self.pool_size\n",
    "        \n",
    "        self.output = np.zeros((batch_size, output_height, output_width, input_channels))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for a in range(0, input_height, self.pool_size):\n",
    "                for b in range(0, input_width, self.pool_size):\n",
    "                    for c in range(input_channels):\n",
    "                        self.output[i, a // self.pool_size, b // self.pool_size, c] = np.max(inputs[i, a:a + self.pool_size, b:b + self.pool_size, c])\n",
    "        \n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d068b698",
   "metadata": {},
   "outputs": [],
   "source": [
    "class flattenLayer:\n",
    "    def forward(self,inputs):\n",
    "        self.inputs_shape=inputs.shape\n",
    "        return inputs.reshape(inputs.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9e3a2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "class denseLayer:\n",
    "    def __init__(self,num_units,activation):\n",
    "        self.num_units = num_units\n",
    "        self.activation = activation\n",
    "        self.weights = np.random.randn(num_units) * 0.01\n",
    "        self.bias = np.zeros((num_units,))\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        self.inputs= inputs\n",
    "        self.output= np.dot(inputs,self.weights)+self.bias\n",
    "        self.output= self.activation(self.output)\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd43c36a",
   "metadata": {},
   "source": [
    "Creating the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d9e2e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=[\n",
    "    convLayer(32,3,channels,relu),\n",
    "    maxpoolLayer(2),\n",
    "    convLayer(64,3,channels,relu),\n",
    "    maxpoolLayer(2),\n",
    "    convLayer(64,3,channels,relu),\n",
    "    maxpoolLayer(2),\n",
    "    convLayer(64,3,channels,relu),\n",
    "    maxpoolLayer(2),\n",
    "    convLayer(64,3,channels,relu),\n",
    "    maxpoolLayer(2),\n",
    "    flattenLayer(),\n",
    "    denseLayer(64,relu),\n",
    "    denseLayer(len(class_names),softmax)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5272b311",
   "metadata": {},
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bff73971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_crossentropy(predictions,targets):\n",
    "    epsilon= 1e-15\n",
    "    predictions= np.clip(predictions, epsilon, 1-epsilon)\n",
    "    n= predictions.shape[0]\n",
    "    loss= -np.sum(targets* np.log(predictions+1e-9))/n\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf9942d",
   "metadata": {},
   "source": [
    "Gradient Descent Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "682e632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(params,gradients,learning_rate=0.001):\n",
    "    for p,g in zip(params,gradients):\n",
    "        p-=learning_rate*g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6af743",
   "metadata": {},
   "source": [
    "Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3ed0fa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9317fd4f",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a00a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution(input_data, filters, stride=1, padding='valid'):\n",
    "    if padding == 'valid':\n",
    "        output_height = (input_data.shape[0] - filters.shape[0]) // stride + 1\n",
    "        output_width = (input_data.shape[1] - filters.shape[1]) // stride + 1\n",
    "    elif padding == 'same':\n",
    "        output_height = input_data.shape[0]\n",
    "        output_width = input_data.shape[1]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid padding mode. Use 'valid' or 'same'.\")\n",
    "\n",
    "    output = np.zeros((output_height, output_width))\n",
    "\n",
    "    for i in range(0, input_data.shape[0] - filters.shape[0] + 1, stride):\n",
    "        for j in range(0, input_data.shape[1] - filters.shape[1] + 1, stride):\n",
    "            output[i, j] = np.sum(input_data[i:i+filters.shape[0], j:j+filters.shape[1]] * filters)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Define a function for a custom convolutional layer\n",
    "def custom_conv_layer(input_data, filters, bias, stride=1, padding='valid'):\n",
    "    # Perform convolution operation and apply activation (e.g., ReLU)\n",
    "    conv_result = convolution(input_data, filters, stride, padding)\n",
    "    activation_result = relu(conv_result)  # Apply ReLU activation\n",
    "    return activation_result\n",
    "\n",
    "# Define a function for a custom dense (fully connected) layer\n",
    "def custom_dense_layer(input_data, weights, bias):\n",
    "    # Perform matrix multiplication and apply activation (e.g., ReLU)\n",
    "    dense_result = np.dot(input_data, weights) + bias\n",
    "    activation_result = relu(dense_result)  # Apply ReLU activation\n",
    "    return activation_result\n",
    "\n",
    "# Define a function to compute gradients for a layer\n",
    "def compute_layer_gradients(layer, prev_activation, delta):\n",
    "    # Compute gradients for weights and bias\n",
    "    if hasattr(layer, 'weights') and hasattr(layer, 'bias'):\n",
    "        gradients = {\n",
    "            'weights': np.dot(prev_activation.T, delta),\n",
    "            'bias': np.sum(delta, axis=0)\n",
    "        }\n",
    "    else:\n",
    "        gradients = None\n",
    "    return gradients\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    num_batches = 0  # Initialize a counter for the number of batches processed in this epoch\n",
    "    \n",
    "    for x_batch, y_batch in train_dg:\n",
    "        # Forward pass through your model\n",
    "        output = x_batch\n",
    "        activations = [output]  # List to store layer activations\n",
    "        \n",
    "        for layer in model:\n",
    "            output = layer.forward(output)\n",
    "            activations.append(output)\n",
    "        \n",
    "        # Calculate the categorical cross-entropy loss\n",
    "        loss = categorical_crossentropy(output, y_batch)\n",
    "        total_loss += loss\n",
    "        num_batches += 1  # Increment the batch counter\n",
    "        \n",
    "        # Compute gradients using backpropagation\n",
    "        delta = output - y_batch  # Gradient of loss with respect to the output of the last layer\n",
    "        all_gradients = []\n",
    "        \n",
    "        for i in reversed(range(len(model))):\n",
    "            layer = model[i]\n",
    "            prev_activation = activations[i]\n",
    "            \n",
    "            # Compute gradients for the current layer\n",
    "            layer_gradients = compute_layer_gradients(layer, prev_activation, delta)\n",
    "            all_gradients.append(layer_gradients)\n",
    "            \n",
    "            # Compute gradient of loss with respect to the input of the current layer\n",
    "            delta = layer.backward(delta)\n",
    "        \n",
    "        # Reverse the gradients list to match the layer order\n",
    "        all_gradients.reverse()\n",
    "        \n",
    "        # Update model parameters\n",
    "        for layer, layer_gradients in zip(model, all_gradients):\n",
    "            if layer_gradients:\n",
    "                layer.weights -= learning_rate * layer_gradients['weights']\n",
    "                layer.bias -= learning_rate * layer_gradients['bias']\n",
    "\n",
    "    # Calculate and print average loss for this epoch\n",
    "    average_loss = total_loss / num_batches  # Use num_batches to calculate average loss\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857bf80a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
