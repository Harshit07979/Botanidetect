{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15893352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy for linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib for plotting the loss functions and/or accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# show progress bar\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec6d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "\n",
    "    def __init__(self, activation_type=None):\n",
    "        if activation_type is None:\n",
    "            self.activation_type = 'linear'\n",
    "        else:\n",
    "            self.activation_type = activation_type\n",
    "\n",
    "    def linear(self, x):\n",
    "        return x\n",
    "\n",
    "    def d_linear(self, x):\n",
    "        return np.ones(x.shape)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def d_sigmoid(self, x):\n",
    "        return self.sigmoid(x) * (1-self.sigmoid(x))\n",
    "     \n",
    "    def tanh(self, x):\n",
    "        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "    def d_tanh(self, x):\n",
    "        return 1-(self.tanh(x))**2\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        return x * (x > 0)\n",
    "\n",
    "    def d_ReLU(self, x):\n",
    "        return (x>0)*np.ones(x.shape)\n",
    "\n",
    "    def PReLU(self, x, alpha=0.2):\n",
    "        return np.where(x > 0, x, alpha*x) \n",
    "\n",
    "    def d_PReLU(self, x, alpha=0.2):\n",
    "        return np.where(x > 0, 1, alpha) \n",
    "\n",
    "    def softmax(self, x):\n",
    "        z = x - np.max(x, axis=-1, keepdims=True)\n",
    "        numerator = np.exp(z)\n",
    "        denominator = np.sum(numerator, axis=-1, keepdims=True)\n",
    "        softmax = numerator / denominator\n",
    "        return softmax\n",
    "\n",
    "    def d_softmax(self, x):\n",
    "        if len(x.shape)==1:\n",
    "            x = np.array(x).reshape(1,-1)\n",
    "        else:\n",
    "            x = np.array(x)\n",
    "        m, d = x.shape\n",
    "        a = self.softmax(x)\n",
    "        tensor1 = np.einsum('ij,ik->ijk', a, a)\n",
    "        tensor2 = np.einsum('ij,jk->ijk', a, np.eye(d, d))\n",
    "        return tensor2 - tensor1\n",
    "\n",
    "    def get_activation(self, x):\n",
    "        if self.activation_type == 'sigmoid':\n",
    "            return self.sigmoid(x)\n",
    "        elif self.activation_type == 'tanh':\n",
    "            return self.tanh(x)\n",
    "        elif self.activation_type == 'relu':\n",
    "            return self.ReLU(x)\n",
    "        elif self.activation_type == 'linear':\n",
    "            return self.linear(x)\n",
    "        elif self.activation_type == 'prelu':\n",
    "            return self.PReLU(x)\n",
    "        elif self.activation_type == 'softmax':\n",
    "            return self.softmax(x)\n",
    "        else:\n",
    "            raise ValueError(\"Valid Activations are only 'sigmoid', 'linear', 'tanh' 'softmax', 'prelu' and 'relu'\")\n",
    "\n",
    "    def get_d_activation(self, x):\n",
    "        if self.activation_type == 'sigmoid':\n",
    "            return self.d_sigmoid(x)\n",
    "        elif self.activation_type == 'tanh':\n",
    "            return self.d_tanh(x)\n",
    "        elif self.activation_type == 'relu':\n",
    "            return self.d_ReLU(x)\n",
    "        elif self.activation_type == 'linear':\n",
    "            return self.d_linear(x)\n",
    "        elif self.activation_type == 'prelu':\n",
    "            return self.d_PReLU(x)\n",
    "        elif self.activation_type == 'softmax':\n",
    "            return self.d_softmax(x)\n",
    "        else:\n",
    "            raise ValueError(\"Valid Activations are only 'sigmoid', 'linear', 'tanh', 'softmax', 'prelu' and 'relu'\")\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        z = self.get_activation(X)\n",
    "        return z\n",
    "    \n",
    "    def backpropagation(self, dz):\n",
    "        f_prime = self.get_d_activation(self.X)\n",
    "        if self.activation_type=='softmax':\n",
    "            # because derivative of softmax is a tensor\n",
    "            dx = np.einsum('ijk,ik->ij', f_prime, dz)\n",
    "        else:\n",
    "            dx = dz * f_prime\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7939c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cost:\n",
    "\n",
    "    def __init__(self, cost_type='mse'):\n",
    "        self.cost_type = cost_type\n",
    "\n",
    "    def mse(self, a, y):\n",
    "        return (1/2)*np.sum((np.linalg.norm(a-y, axis=1))**2)\n",
    "\n",
    "    def d_mse(self, a, y):\n",
    "        return a - y\n",
    "\n",
    "    def cross_entropy(self, a, y, epsilon=1e-12):\n",
    "        a = np.clip(a, epsilon, 1. - epsilon)\n",
    "        return -np.sum(y*np.log(a))\n",
    "\n",
    "    def d_cross_entropy(self, a, y, epsilon=1e-12):\n",
    "        a = np.clip(a, epsilon, 1. - epsilon)\n",
    "        return -y/a\n",
    "\n",
    "    def get_cost(self, a, y):\n",
    "        if self.cost_type == 'mse':\n",
    "            return self.mse(a, y)\n",
    "        elif self.cost_type == 'cross-entropy':\n",
    "            return self.cross_entropy(a, y)\n",
    "        else:\n",
    "            raise ValueError(\"Valid cost functions are only 'mse', and 'cross-entropy'\")\n",
    "\n",
    "    def get_d_cost(self, a, y):\n",
    "        if self.cost_type == 'mse':\n",
    "            return self.d_mse(a, y)\n",
    "        elif self.cost_type == 'cross-entropy':\n",
    "            return self.d_cross_entropy(a, y)\n",
    "        else:\n",
    "            raise ValueError(\"Valid cost functions are only 'mse', and 'cross-entropy'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5add9f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "\n",
    "    def __init__(self, optimizer_type=None, shape_W=None, shape_b=None,\n",
    "                 momentum1=0.9, momentum2=0.999, epsilon=1e-8):\n",
    "        if optimizer_type is None:\n",
    "            self.optimizer_type = 'adam'\n",
    "        else:\n",
    "            self.optimizer_type = optimizer_type\n",
    "\n",
    "        self.momentum1 = momentum1\n",
    "        self.momentum2 = momentum2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.vdW = np.zeros(shape_W)\n",
    "        self.vdb = np.zeros(shape_b)\n",
    "\n",
    "        self.SdW = np.zeros(shape_W)\n",
    "        self.Sdb = np.zeros(shape_b)\n",
    "\n",
    "    def GD(self, dW, db, k):\n",
    "        return dW, db\n",
    " \n",
    "    def SGD(self, dW, db, k):\n",
    "        self.vdW = self.momentum1*self.vdW + (1-self.momentum1)*dW\n",
    "        self.vdb = self.momentum1*self.vdb + (1-self.momentum1)*db\n",
    "\n",
    "        return self.vdW, self.vdb\n",
    "\n",
    "    def RMSProp(self, dW, db, k):\n",
    "        self.SdW = self.momentum2*self.SdW + (1-self.momentum2)*(dW**2)\n",
    "        self.Sdb = self.momentum2*self.Sdb + (1-self.momentum2)*(db**2)\n",
    "\n",
    "        den_W = np.sqrt(self.SdW) + self.epsilon\n",
    "        den_b = np.sqrt(self.Sdb) + self.epsilon\n",
    "\n",
    "        return dW/den_W, db/den_b\n",
    " \n",
    "    def Adam(self, dW, db, k):\n",
    "        # momentum\n",
    "        self.vdW = self.momentum1*self.vdW + (1-self.momentum1)*dW\n",
    "        self.vdb = self.momentum1*self.vdb + (1-self.momentum1)*db\n",
    "\n",
    "        # rmsprop\n",
    "        self.SdW = self.momentum2*self.SdW + (1-self.momentum2)*(dW**2)\n",
    "        self.Sdb = self.momentum2*self.Sdb + (1-self.momentum2)*(db**2)\n",
    "\n",
    "        # correction\n",
    "        if k>1:\n",
    "            vdW_h = self.vdW / (1-(self.momentum1**k))\n",
    "            vdb_h = self.vdb / (1-(self.momentum1**k))\n",
    "            SdW_h = self.SdW / (1-(self.momentum2**k))\n",
    "            Sdb_h = self.Sdb / (1-(self.momentum2**k))\n",
    "        else:\n",
    "            vdW_h = self.vdW \n",
    "            vdb_h = self.vdb\n",
    "            SdW_h = self.SdW\n",
    "            Sdb_h = self.Sdb\n",
    "\n",
    "        den_W = np.sqrt(SdW_h) + self.epsilon\n",
    "        den_b = np.sqrt(Sdb_h) + self.epsilon\n",
    "\n",
    "        return vdW_h/den_W, vdb_h/den_b\n",
    "\n",
    "    def get_optimization(self, dW, db, k):\n",
    "        if self.optimizer_type == 'gd':\n",
    "            return self.GD(dW, db, k)\n",
    "        if self.optimizer_type == 'sgd':\n",
    "            return self.SGD(dW, db, k)\n",
    "        if self.optimizer_type == 'rmsprop':\n",
    "            return self.RMSProp(dW, db, k)\n",
    "        if self.optimizer_type == 'adam':\n",
    "            return self.Adam(dW, db, k)\n",
    "        else:\n",
    "            raise ValueError(\"Valid optimizer options are only 'gd', 'sgd', 'rmsprop', and 'adam'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2d8243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateDecay:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def constant(self, t, lr_0):\n",
    "        return lr_0\n",
    "\n",
    "    def time_decay(self, t, lr_0, k):\n",
    "        lr = lr_0 /(1+(k*t))\n",
    "        return lr\n",
    "\n",
    "    def step_decay(self, t, lr_0, F, D):\n",
    "        mult = F**np.floor((1+t)/D)\n",
    "        lr = lr_0 * mult\n",
    "        return lr\n",
    "\n",
    "    def exponential_decay(self, t, lr_0, k):\n",
    "        lr = lr_0 * np.exp(-k*t)\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58a36d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Utility:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def label_encoding(self, Y):\n",
    "        idx_list = []\n",
    "        result = []\n",
    "        for col in range(Y.shape[1]):\n",
    "            indexes = {val: idx for idx, val in enumerate(np.unique(Y[:, col]))}\n",
    "            result.append([indexes[s] for s in Y[:, col]])\n",
    "            idx_list.append(indexes)\n",
    "        return np.array(result).T, idx_list\n",
    "\n",
    "    def onehot(self, X):\n",
    "        indexes = {val: idx for idx, val in enumerate(np.unique(X))}\n",
    "        y = np.array([indexes[s] for s in X])\n",
    "        X_onehot = np.zeros((y.size, len(indexes)))\n",
    "        X_onehot[np.arange(y.size), y] = 1\n",
    "        return X_onehot, indexes\n",
    "\n",
    "    def minmax(self, X, min_X=None, max_X=None):\n",
    "        if min_X is None:\n",
    "            min_X = np.min(X, axis=0)\n",
    "        if max_X is None:\n",
    "            max_X = np.max(X, axis=0)\n",
    "        Z = (X - min_X) / (max_X - min_X)\n",
    "        return Z, min_X, max_X\n",
    "\n",
    "    def standardize(self, X, mu=None, std=None):\n",
    "        if mu is None:\n",
    "            mu = np.mean(X, axis=0)\n",
    "        if std is None:\n",
    "            std = np.std(X, axis=0)\n",
    "        Z = (X - mu) / std\n",
    "        return Z, mu, std\n",
    "\n",
    "    def inv_standardize(self, Z, mu, std):\n",
    "        X = Z*std + mu\n",
    "        return X\n",
    "\n",
    "    def train_test_split(self, X, y, test_ratio=0.2, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        train_ratio = 1-test_ratio\n",
    "        indices = np.random.permutation(X.shape[0])\n",
    "        train_idx, test_idx = indices[:int(train_ratio*len(X))], indices[int(train_ratio*len(X)):]\n",
    "        X_train, X_test = X[train_idx,:], X[test_idx,:]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4dd35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weights_initializer:\n",
    "\n",
    "    def __init__(self, shape, initializer_type=None, seed=None):\n",
    "        self.shape = shape\n",
    "        if initializer_type is None:\n",
    "            self.initializer_type = \"he_normal\"\n",
    "        else:\n",
    "            self.initializer_type = initializer_type\n",
    "        self.seed = seed\n",
    "        \n",
    "    def zeros_initializer(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        return np.zeros(self.shape)\n",
    "\n",
    "    def ones_initializer(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        return np.ones(self.shape)\n",
    "\n",
    "    def random_normal_initializer(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        return np.random.normal(size=self.shape)\n",
    "\n",
    "    def random_uniform_initializer(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        return np.random.uniform(size=self.shape)\n",
    "\n",
    "    def he_initializer(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        try:\n",
    "            F, Kc, Kh, Kw = self.shape\n",
    "        except:\n",
    "            Kh, Kw = self.shape\n",
    "        return np.random.randn(*self.shape) * np.sqrt(2/Kh)\n",
    "\n",
    "    def xavier_initializer(self):\n",
    "        '''\n",
    "        shape: Shape of the Kernel matrix.\n",
    "        '''\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        try:\n",
    "            F, Kc, Kh, Kw = self.shape\n",
    "        except:\n",
    "            Kh, Kw = self.shape\n",
    "        return np.random.randn(*self.shape) * np.sqrt(1/Kh)\n",
    "\n",
    "    def glorot_initializer(self):\n",
    "        '''\n",
    "        shape: Shape of the weight matrix.\n",
    "        '''\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        try:\n",
    "            F, Kc, Kh, Kw = self.shape\n",
    "        except:\n",
    "            Kh, Kw = self.shape\n",
    "        return np.random.randn(*self.shape) * np.sqrt(2/(Kh+Kw))\n",
    "\n",
    "    def get_initializer(self):\n",
    "        if self.initializer_type == 'zeros':\n",
    "            return self.zeros_initializer()\n",
    "        elif self.initializer_type == 'ones':\n",
    "            return self.ones_initializer()\n",
    "        elif self.initializer_type == 'random_normal':\n",
    "            return self.random_normal_initializer()\n",
    "        elif self.initializer_type == 'random_uniform':\n",
    "            return self.random_uniform_initializer()\n",
    "        elif self.initializer_type == 'he_normal':\n",
    "            return self.he_initializer()\n",
    "        elif self.initializer_type == 'xavier_normal':\n",
    "            return self.xavier_initializer()\n",
    "        elif self.initializer_type == 'glorot_normal':\n",
    "            return self.glorot_initializer()\n",
    "        else:\n",
    "            raise ValueError(\"Valid initializer options are 'zeros', 'ones', 'random_normal', 'random_uniform', 'he_normal', 'xavier_normal', and 'glorot_normal'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17d88bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "\n",
    "    def __init__(self, neurons, activation_type=None, use_bias=True, \n",
    "                 weight_initializer_type=None, weight_regularizer=None, seed=None, input_dim=None):\n",
    "\n",
    "        self.neurons = neurons\n",
    "        self.activation = Activation(activation_type=activation_type)\n",
    "        self.use_bias = use_bias\n",
    "        self.weight_initializer_type = weight_initializer_type # none is handled\n",
    "        if weight_regularizer is None:\n",
    "            self.weight_regularizer = ('L2', 0)\n",
    "        else:\n",
    "            self.weight_regularizer = weight_regularizer\n",
    "        self.seed = seed\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "\n",
    "    def initialize_parameters(self, hl, optimizer_type):\n",
    "        shape_W = (hl, self.neurons)\n",
    "        shape_b = (self.neurons, 1)\n",
    "        initializer = Weights_initializer(shape=shape_W,\n",
    "                                          initializer_type=self.weight_initializer_type,\n",
    "                                          seed=self.seed)\n",
    "        self.W = initializer.get_initializer()\n",
    "        self.b = np.zeros(shape_b)\n",
    "        \n",
    "        self.optimizer = Optimizer(optimizer_type=optimizer_type, shape_W=shape_W, shape_b=shape_b)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        r = X @ self.W\n",
    "        self.z = r + self.b.T\n",
    "        a = self.activation.forward(self.z)\n",
    "        return a\n",
    "    \n",
    "    def backpropagation(self, da):\n",
    "        dz = self.activation.backpropagation(da)\n",
    "        dr = dz.copy()\n",
    "        self.db = np.sum(dz, axis=0).reshape(-1,1)\n",
    "        self.dW = (self.X.T) @ dr\n",
    "        dX = dr @ (self.W.T)\n",
    "        return dX\n",
    "\n",
    "    def update(self, lr, m, k):\n",
    "        dW, db = self.optimizer.get_optimization(self.dW, self.db, k)\n",
    "\n",
    "        if self.weight_regularizer[0].lower()=='l2':\n",
    "            dW += self.weight_regularizer[1] * self.W\n",
    "        elif self.weight_regularizer[0].lower()=='l1':\n",
    "            dW += self.weight_regularizer[1] * np.sign(self.W)\n",
    "        \n",
    "        self.W -= dW*(lr/m)\n",
    "        if self.use_bias:\n",
    "            self.b -= db*(lr/m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c9f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "        if self.p == 0:\n",
    "            self.p += 1e-6\n",
    "        if self.p == 1:\n",
    "            self.p -= 1e-6\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.mask = (np.random.rand(*X.shape) < self.p) / self.p \n",
    "        Z = X * self.mask\n",
    "        return Z\n",
    "    \n",
    "    def backpropagation(self, dZ):\n",
    "        dX = dZ * self.mask\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "604432cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "\n",
    "    def __init__(self, momentum=0.9, epsilon=1e-6):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    def initialize_parameters(self, d):\n",
    "        self.gamma = np.ones((d))\n",
    "        self.beta = np.zeros((d))\n",
    "        self.running_mean = np.zeros((d))\n",
    "        self.running_var = np.zeros((d))\n",
    "    \n",
    "    def forward(self, z, mode='train'):\n",
    "        if mode=='train':\n",
    "            self.m, self.d = z.shape\n",
    "            self.mu = np.mean(z, axis = 0) # ðœ‡\n",
    "            self.var = np.var(z, axis=0) # ðœŽ^2\n",
    "            self.zmu = z - self.mu # z - ðœ‡\n",
    "            self.ivar = 1 / np.sqrt(self.var + self.epsilon) # ðœŽð‘–ð‘›ð‘£\n",
    "            self.zhat = self.zmu * self.ivar \n",
    "            q = self.gamma*self.zhat + self.beta # ql\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mu\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
    "        elif mode=='test':\n",
    "            q = (z - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            q = self.gamma*q + self.beta\n",
    "        else:\n",
    "            raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
    "        return q\n",
    "\n",
    "    def backpropagation(self, dq):\n",
    "        self.dgamma = np.sum(dq * self.zhat, axis=0)\n",
    "        self.dbeta = np.sum(dq, axis=0)\n",
    "        dzhat = dq * self.gamma\n",
    "        dvar = np.sum(dzhat * self.zmu * (-.5) * (self.ivar**3), axis=0)\n",
    "        dmu = np.sum(dzhat * (-self.ivar), axis=0)\n",
    "        dz = dzhat * self.ivar + dvar * (2/self.m) * self.zmu + (1/self.m)*dmu\n",
    "        return dz\n",
    "\n",
    "    def update(self, lr, m, k):\n",
    "        self.gamma -= self.dgamma*(lr/m)\n",
    "        self.beta -= self.dbeta*(lr/m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bee72671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding2D:\n",
    "\n",
    "    def __init__(self, p='valid'):\n",
    "        self.p = p\n",
    "        \n",
    "    def get_dimensions(self, input_shape, kernel_size, s=(1,1)):\n",
    "        if len(input_shape)==4:\n",
    "            m, Nc, Nh, Nw = input_shape\n",
    "        elif len(input_shape)==3:\n",
    "            Nc, Nh, Nw = input_shape\n",
    "        \n",
    "        Kh, Kw = kernel_size\n",
    "        sh, sw = s\n",
    "        p = self.p\n",
    "        \n",
    "        if type(p)==int:\n",
    "            pt, pb = p, p\n",
    "            pl, pr = p, p\n",
    "\n",
    "        if type(p)==tuple:\n",
    "            ph, pw = p\n",
    "            pt, pb = ph//2, (ph+1)//2\n",
    "            pl, pr = pw//2, (pw+1)//2\n",
    "\n",
    "        elif p=='valid':\n",
    "            pt, pb = 0, 0\n",
    "            pl, pr = 0, 0\n",
    "\n",
    "        elif p=='same':\n",
    "            # calculating how much padding is required in all 4 directions \n",
    "            # (top, bottom, left and right)\n",
    "            ph = (sh-1)*Nh + Kh - sh\n",
    "            pw = (sw-1)*Nw + Kw - sw\n",
    "\n",
    "            pt, pb = ph//2, (ph+1)//2\n",
    "            pl, pr = pw//2, (pw+1)//2\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Incorrect padding type. Allowed types are only 'same', 'valid', an integer or a tuple of length 2.\")\n",
    "            \n",
    "        if len(input_shape)==4:\n",
    "            output_shape = (m, Nc, Nh+pt+pb, Nw+pl+pr)\n",
    "        elif len(input_shape)==3:\n",
    "            output_shape = (Nc, Nh+pt+pb, Nw+pl+pr)\n",
    "        \n",
    "        return output_shape, (pt, pb, pl, pr)\n",
    "        \n",
    "    def forward(self, X, kernel_size, s=(1,1)):\n",
    "        self.input_shape = X.shape\n",
    "        m, Nc, Nh, Nw = self.input_shape\n",
    "        \n",
    "        self.output_shape, (self.pt, self.pb, self.pl, self.pr) = self.get_dimensions(self.input_shape, \n",
    "                                                                                      kernel_size, s=s)\n",
    "        \n",
    "        zeros_r = np.zeros((m, Nc, Nh, self.pr))\n",
    "        zeros_l = np.zeros((m, Nc, Nh, self.pl))\n",
    "        zeros_t = np.zeros((m, Nc, self.pt, Nw + self.pl + self.pr))\n",
    "        zeros_b = np.zeros((m, Nc, self.pb, Nw + self.pl + self.pr))\n",
    "\n",
    "        Xp = np.concatenate((X, zeros_r), axis=3)\n",
    "        Xp = np.concatenate((zeros_l, Xp), axis=3)\n",
    "        Xp = np.concatenate((zeros_t, Xp), axis=2)\n",
    "        Xp = np.concatenate((Xp, zeros_b), axis=2)\n",
    "        \n",
    "        return Xp\n",
    "    \n",
    "    def backpropagation(self, dXp):\n",
    "        m, Nc, Nh, Nw = self.input_shape\n",
    "        dX = dXp[:, :, self.pt:self.pt+Nh, self.pl:self.pl+Nw]\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccd69987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "\n",
    "    def __init__(self, filters, kernel_size, s=(1, 1), p='valid',\n",
    "                 activation_type=None, use_bias=True, weight_initializer_type=None, \n",
    "                 kernel_regularizer=None, seed=None, input_shape=None):\n",
    "\n",
    "        self.padding = Padding2D(p=p)\n",
    "        \n",
    "        self.F = filters\n",
    "        \n",
    "        self.input_shape_x = input_shape\n",
    "        \n",
    "        if type(kernel_size)==int:\n",
    "            self.kernel_size = (kernel_size, kernel_size)\n",
    "        elif type(kernel_size)==tuple and len(kernel_size)==2:\n",
    "            self.kernel_size = kernel_size\n",
    "            \n",
    "        self.Kh, self.Kw  = self.kernel_size\n",
    "        \n",
    "        if type(s)==int:\n",
    "            self.s = (s,s)\n",
    "        elif type(s)==tuple and len(s)==2:\n",
    "            self.s = s\n",
    "        \n",
    "        self.sh, self.sw = self.s\n",
    "        \n",
    "        self.activation = Activation(activation_type=activation_type)\n",
    "        self.use_bias = use_bias\n",
    "        self.weight_initializer_type = weight_initializer_type # none is handled\n",
    "        if kernel_regularizer is None:\n",
    "            self.kernel_regularizer = ('L2', 0)\n",
    "        else:\n",
    "            self.kernel_regularizer = kernel_regularizer\n",
    "        self.seed = seed\n",
    "        \n",
    "    def get_dimensions(self, input_shape):\n",
    "        \n",
    "        self.input_shape_x = input_shape # (3D or 4D)\n",
    "        \n",
    "        # Padded X will be actual input to this Conv2D\n",
    "        \n",
    "        self.input_shape, _ = self.padding.get_dimensions(self.input_shape_x, \n",
    "                                                          self.kernel_size, self.s)\n",
    "        \n",
    "        if len(input_shape)==3:\n",
    "            self.Nc, self.Nh, self.Nw = self.input_shape\n",
    "        elif len(input_shape)==4:\n",
    "            self.m, self.Nc, self.Nh, self.Nw = self.input_shape\n",
    "        \n",
    "        # Output shape\n",
    "        self.Oh = (self.Nh - self.Kh)//self.sh + 1\n",
    "        self.Ow = (self.Nw - self.Kw)//self.sw + 1\n",
    "        \n",
    "        if len(input_shape)==3:\n",
    "            self.output_shape = (self.F, self.Oh, self.Ow)\n",
    "        elif len(input_shape)==4:\n",
    "            self.output_shape = (self.m, self.F, self.Oh, self.Ow)\n",
    "\n",
    "    def initialize_parameters(self, input_shape, optimizer_type):\n",
    "        \n",
    "        self.get_dimensions(input_shape)\n",
    "        \n",
    "        shape_b = (self.F, self.Oh, self.Ow)\n",
    "        \n",
    "        shape_K = (self.F, self.Nc, self.Kh, self.Kw)\n",
    "        \n",
    "        initializer = Weights_initializer(shape=shape_K,\n",
    "                                          initializer_type=self.weight_initializer_type,\n",
    "                                          seed=self.seed)\n",
    "        \n",
    "        self.K = initializer.get_initializer()\n",
    "        self.b = np.zeros(shape_b)\n",
    "        \n",
    "        self.optimizer = Optimizer(optimizer_type=optimizer_type, shape_W=shape_K, shape_b=shape_b)\n",
    "    \n",
    "    def dilate2D(self, X, Dr=(1,1)):\n",
    "        dh, dw = Dr # Dilate rate\n",
    "        m, C, H, W = X.shape\n",
    "        Xd = np.insert(arr=X, obj=np.repeat(np.arange(1,W), dw-1), values=0, axis=-1)\n",
    "        Xd = np.insert(arr=Xd, obj=np.repeat(np.arange(1,H), dh-1), values=0, axis=-2)\n",
    "        return Xd\n",
    "\n",
    "    def prepare_subMatrix(self, X, Kh, Kw, s):\n",
    "        m, Nc, Nh, Nw = X.shape\n",
    "        sh, sw = s\n",
    "\n",
    "        Oh = (Nh-Kh)//sh + 1\n",
    "        Ow = (Nw-Kw)//sw + 1\n",
    "\n",
    "        strides = (Nc*Nh*Nw, Nw*Nh, Nw*sh, sw, Nw, 1)\n",
    "        strides = tuple(i * X.itemsize for i in strides)\n",
    "\n",
    "        subM = np.lib.stride_tricks.as_strided(X, \n",
    "                                               shape=(m, Nc, Oh, Ow, Kh, Kw),\n",
    "                                               strides=strides)\n",
    "\n",
    "        return subM\n",
    "         \n",
    "    def convolve(self, X, K, s=(1,1), mode='front'):\n",
    "\n",
    "        F, Kc, Kh, Kw = K.shape\n",
    "        subM = self.prepare_subMatrix(X, Kh, Kw, s)\n",
    "        \n",
    "        if mode=='front':\n",
    "            return np.einsum('fckl,mcijkl->mfij', K, subM)\n",
    "        elif mode=='back':\n",
    "            return np.einsum('fdkl,mcijkl->mdij', K, subM)\n",
    "        elif mode=='param':\n",
    "            return np.einsum('mfkl,mcijkl->fcij', K, subM)\n",
    "\n",
    "    def dZ_D_dX(self, dZ_D, Nh, Nw):\n",
    "\n",
    "        # Pad the dilated dZ (dZ_D -> dZ_Dp)\n",
    "\n",
    "        _, _, Hd, Wd = dZ_D.shape\n",
    "\n",
    "        ph = Nh - Hd + self.Kh - 1\n",
    "        pw = Nw - Wd + self.Kw - 1\n",
    "        \n",
    "        padding_back = Padding2D(p=(ph, pw))\n",
    "\n",
    "        dZ_Dp = padding_back.forward(dZ_D, self.kernel_size, self.s)\n",
    "\n",
    "        # Rotate K by 180 degrees\n",
    "        \n",
    "        K_rotated = self.K[:, :, ::-1, ::-1]\n",
    "        \n",
    "        # convolve dZ_Dp with K_rotated\n",
    "        \n",
    "        dXp = self.convolve(dZ_Dp, K_rotated, mode='back')\n",
    "        \n",
    "        dX = self.padding.backpropagation(dXp)\n",
    "\n",
    "        return dX\n",
    "\n",
    "    def forward(self, X):\n",
    "        # padding\n",
    "\n",
    "        self.X = X\n",
    "\n",
    "        Xp = self.padding.forward(X, self.kernel_size, self.s)\n",
    "        \n",
    "        # convolve Xp with K\n",
    "        Z = self.convolve(Xp, self.K, self.s) + self.b\n",
    "    \n",
    "        a = self.activation.forward(Z)\n",
    "        \n",
    "        return a\n",
    "     \n",
    "    def backpropagation(self, da):\n",
    "\n",
    "        Xp = self.padding.forward(self.X, self.kernel_size, self.s)\n",
    "\n",
    "        m, Nc, Nh, Nw = Xp.shape\n",
    "\n",
    "        dZ = self.activation.backpropagation(da)\n",
    "        \n",
    "        # Dilate dZ (dZ-> dZ_D)\n",
    "        \n",
    "        dZ_D = self.dilate2D(dZ, Dr=self.s)\n",
    "        \n",
    "        dX = self.dZ_D_dX(dZ_D, Nh, Nw)\n",
    "        \n",
    "        # Gradient dK\n",
    "        \n",
    "        _, _, Hd, Wd = dZ_D.shape\n",
    "        \n",
    "        ph = self.Nh - Hd - self.Kh + 1\n",
    "        pw = self.Nw - Wd - self.Kw + 1\n",
    "\n",
    "        padding_back = Padding2D(p=(ph, pw))\n",
    "\n",
    "        dZ_Dp = padding_back.forward(dZ_D, self.kernel_size, self.s)\n",
    "        \n",
    "        self.dK = self.convolve(Xp, dZ_Dp, mode='param')\n",
    "        \n",
    "        # Gradient db\n",
    "        \n",
    "        self.db = np.sum(dZ, axis=0)\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    def update(self, lr, m, k):\n",
    "        dK, db = self.optimizer.get_optimization(self.dK, self.db, k)\n",
    "\n",
    "        if self.kernel_regularizer[0].lower()=='l2':\n",
    "            dK += self.kernel_regularizer[1] * self.K\n",
    "        elif self.weight_regularizer[0].lower()=='l1':\n",
    "            dK += self.kernel_regularizer[1] * np.sign(self.K)\n",
    "\n",
    "        self.K -= self.dK*(lr/m)\n",
    "        \n",
    "        if self.use_bias:\n",
    "            self.b -= self.db*(lr/m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b5cc2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling2D:\n",
    "\n",
    "    def __init__(self, pool_size=(2,2), s=(2,2), p='valid', pool_type='max'):\n",
    "        self.padding = Padding2D(p=p)\n",
    "        \n",
    "        if type(pool_size)==int:\n",
    "            self.pool_size = (pool_size, pool_size)\n",
    "        elif type(pool_size)==tuple and len(pool_size)==2:\n",
    "            self.pool_size = pool_size\n",
    "            \n",
    "        self.Kh, self.Kw  = self.pool_size\n",
    "        \n",
    "        if type(s)==int:\n",
    "            self.s = (s,s)\n",
    "        elif type(s)==tuple and len(s)==2:\n",
    "            self.s = s\n",
    "        \n",
    "        self.sh, self.sw = self.s\n",
    "        \n",
    "        self.pool_type = pool_type    \n",
    "\n",
    "    def get_dimensions(self, input_shape):\n",
    "        \n",
    "        if len(input_shape)==4:\n",
    "            m, Nc, Nh, Nw = input_shape\n",
    "        elif len(input_shape)==3:\n",
    "            Nc, Nh, Nw = input_shape\n",
    "\n",
    "        Oh = (Nh-self.Kh)//self.sh + 1\n",
    "        Ow = (Nw-self.Kw)//self.sw + 1\n",
    "        \n",
    "        if len(input_shape)==4:\n",
    "            self.output_shape = (m, Nc, Oh, Ow)\n",
    "        elif len(input_shape)==3:\n",
    "            self.output_shape = (Nc, Oh, Ow)\n",
    "\n",
    "    def prepare_subMatrix(self, X, pool_size, s):\n",
    "        m, Nc, Nh, Nw = X.shape\n",
    "        sh, sw = s\n",
    "        Kh, Kw = pool_size\n",
    "\n",
    "        Oh = (Nh-Kh)//sh + 1\n",
    "        Ow = (Nw-Kw)//sw + 1\n",
    "\n",
    "        strides = (Nc*Nh*Nw, Nh*Nw, Nw*sh, sw, Nw, 1)\n",
    "        strides = tuple(i * X.itemsize for i in strides)\n",
    "\n",
    "        subM = np.lib.stride_tricks.as_strided(X, \n",
    "                                               shape=(m, Nc, Oh, Ow, Kh, Kw),\n",
    "                                               strides=strides)\n",
    "        return subM\n",
    "    \n",
    "    def pooling(self, X, pool_size=(2,2), s=(2,2)):\n",
    "        \n",
    "        subM = self.prepare_subMatrix(X, pool_size, s)\n",
    "        \n",
    "        if self.pool_type=='max':\n",
    "            return np.max(subM, axis=(-2,-1))\n",
    "        elif self.pool_type=='mean':\n",
    "            return np.mean(subM, axis=(-2,-1))\n",
    "        else:\n",
    "            raise ValueError(\"Allowed pool types are only 'max' or 'mean'.\")\n",
    "\n",
    "    def prepare_mask(self, subM, Kh, Kw):\n",
    "        \n",
    "        m, Nc, Oh, Ow, Kh, Kw = subM.shape\n",
    "\n",
    "        a = subM.reshape(-1,Kh*Kw)\n",
    "        idx = np.argmax(a, axis=1)\n",
    "        b = np.zeros(a.shape)\n",
    "        b[np.arange(b.shape[0]), idx] = 1\n",
    "        mask = b.reshape((m, Nc, Oh, Ow, Kh, Kw))\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def mask_dXp(self, mask, Xp, dZ, Kh, Kw):\n",
    "        dA = np.einsum('i,ijk->ijk', dZ.reshape(-1), mask.reshape(-1,Kh,Kw)).reshape(mask.shape)\n",
    "        m, Nc, Nh, Nw = Xp.shape\n",
    "        strides = (Nc*Nh*Nw, Nh*Nw, Nw, 1)\n",
    "        strides = tuple(i * Xp.itemsize for i in strides)\n",
    "        dXp = np.lib.stride_tricks.as_strided(dA, Xp.shape, strides)\n",
    "        return dXp\n",
    "            \n",
    "    def maxpool_backprop(self, dZ, X):\n",
    "        \n",
    "        Xp = self.padding.forward(X, self.pool_size, self.s)\n",
    "\n",
    "        subM = self.prepare_subMatrix(Xp, self.pool_size, self.s)\n",
    "\n",
    "        m, Nc, Oh, Ow, Kh, Kw = subM.shape\n",
    "        \n",
    "        m, Nc, Nh, Nw = Xp.shape\n",
    "\n",
    "        mask = self.prepare_mask(subM, Kh, Kw)\n",
    "        \n",
    "        dXp = self.mask_dXp(mask, Xp, dZ, Kh, Kw)\n",
    "        \n",
    "        return dXp\n",
    "\n",
    "    def dZ_dZp(self, dZ):\n",
    "        sh, sw = self.s\n",
    "        Kh, Kw = self.pool_size\n",
    "\n",
    "        dZp = np.kron(dZ, np.ones((Kh,Kw), dtype=dZ.dtype)) # similar to repelem in matlab\n",
    "\n",
    "        jh, jw = Kh-sh, Kw-sw # jump along height and width\n",
    "\n",
    "        if jw!=0:\n",
    "            L = dZp.shape[-1]-1\n",
    "\n",
    "            l1 = np.arange(sw, L)\n",
    "            l2 = np.arange(sw + jw, L + jw)\n",
    "\n",
    "            mask = np.tile([True]*jw + [False]*jw, len(l1)//jw).astype(bool)\n",
    "\n",
    "            r1 = l1[mask[:len(l1)]]\n",
    "            r2 = l2[mask[:len(l2)]]\n",
    "\n",
    "            dZp[:, :, :, r1] += dZp[:, :, :, r2]\n",
    "            dZp = np.delete(dZp, r2, axis=-1)\n",
    "\n",
    "        if jh!=0:\n",
    "            L = dZp.shape[-2]-1\n",
    "\n",
    "            l1 = np.arange(sh, L)\n",
    "            l2 = np.arange(sh + jh, L + jh)\n",
    "\n",
    "            mask = np.tile([True]*jh + [False]*jh, len(l1)//jh).astype(bool)\n",
    "\n",
    "            r1 = l1[mask[:len(l1)]]\n",
    "            r2 = l2[mask[:len(l2)]]\n",
    "\n",
    "            dZp[:, :, r1, :] += dZp[:, :, r2, :]\n",
    "            dZp = np.delete(dZp, r2, axis=-2)\n",
    "\n",
    "        return dZp\n",
    "\n",
    "    def averagepool_backprop(self, dZ, X):\n",
    "        \n",
    "        Xp = self.padding.forward(X, self.pool_size, self.s)\n",
    "\n",
    "        m, Nc, Nh, Nw = Xp.shape\n",
    "\n",
    "        dZp = self.dZ_dZp(dZ)\n",
    "\n",
    "        ph = Nh - dZp.shape[-2]\n",
    "        pw = Nw - dZp.shape[-1]\n",
    "        \n",
    "        padding_back = Padding2D(p=(ph, pw))\n",
    "        \n",
    "        dXp = padding_back.forward(dZp, s=self.s, kernel_size=self.pool_size)\n",
    "\n",
    "        return dXp / (Nh*Nw)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "\n",
    "        # padding\n",
    "        Xp = self.padding.forward(X, self.pool_size, self.s)\n",
    "\n",
    "        Z = self.pooling(Xp, self.pool_size, self.s)\n",
    "        \n",
    "        return Z\n",
    "    \n",
    "    def backpropagation(self, dZ):\n",
    "        if self.pool_type=='max':\n",
    "            dXp = self.maxpool_backprop(dZ, self.X)\n",
    "        elif self.pool_type=='mean':\n",
    "            dXp = self.averagepool_backprop(dZ, self.X)\n",
    "        dX = self.padding.backpropagation(dXp)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a7794c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.m, self.Nc, self.Nh, self.Nw = X.shape\n",
    "        X_flat = X.reshape((self.m, self.Nc * self.Nh * self.Nw))\n",
    "        return X_flat\n",
    "     \n",
    "    def backpropagation(self, dZ):\n",
    "        dX = dZ.reshape((self.m, self.Nc, self.Nh, self.Nw))\n",
    "        return dX\n",
    "    \n",
    "    def get_dimensions(self, input_shape):\n",
    "        \n",
    "        if len(input_shape)==4:\n",
    "            self.m, self.Nc, self.Nh, self.Nw = input_shape\n",
    "        elif len(input_shape)==3:\n",
    "            self.Nc, self.Nh, self.Nw = input_shape\n",
    "        \n",
    "        self.output_shape = self.Nc * self.Nh * self.Nw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbce0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "\n",
    "    def __init__(self, layers=None):\n",
    "        if layers is None:\n",
    "            self.layers = []\n",
    "        else:\n",
    "            self.layers = layers\n",
    "        self.network_architecture_called = False\n",
    "\n",
    "    def add(self, layer):\n",
    "        # adds a layer to CNN model\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def Input(self, input_shape):\n",
    "        self.d = input_shape\n",
    "        self.architecture = [self.d] # output architecture\n",
    "        self.layer_name = [\"Input\"]\n",
    "    \n",
    "    def network_architecture(self):\n",
    "        for layer in self.layers:\n",
    "            if layer.__class__.__name__=='Conv2D':\n",
    "                if layer.input_shape_x is not None:\n",
    "                    self.Input(layer.input_shape_x)\n",
    "                layer.get_dimensions(self.architecture[-1])\n",
    "                self.architecture.append(layer.output_shape)\n",
    "                self.layer_name.append(layer.__class__.__name__)\n",
    "            elif layer.__class__.__name__ in ['Flatten', 'Pooling2D']:\n",
    "                layer.get_dimensions(self.architecture[-1])\n",
    "                self.architecture.append(layer.output_shape)\n",
    "                self.layer_name.append(layer.__class__.__name__)\n",
    "            elif layer.__class__.__name__=='Dense':\n",
    "                self.architecture.append(layer.neurons)\n",
    "                self.layer_name.append(layer.__class__.__name__)\n",
    "            else:\n",
    "                self.architecture.append(self.architecture[-1])\n",
    "                self.layer_name.append(layer.__class__.__name__)\n",
    "\n",
    "        self.layers = [layer for layer in self.layers if layer is not None]\n",
    "        try:\n",
    "            idx = model.layer_name.index(\"NoneType\")\n",
    "            del model.layer_name[idx]\n",
    "            del model.architecture[idx]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def summary(self):\n",
    "        if self.network_architecture_called==False:\n",
    "            self.network_architecture()\n",
    "            self.network_architecture_called = True\n",
    "        len_assigned = [45, 26, 15]\n",
    "        count = {'Dense': 1, 'Activation': 1, 'Input': 1,\n",
    "                'BatchNormalization': 1, 'Dropout': 1, 'Conv2D': 1,\n",
    "                'Pooling2D': 1, 'Flatten': 1}\n",
    "\n",
    "        col_names = ['Layer (type)', 'Output Shape', '# of Parameters']\n",
    "\n",
    "        print(\"Model: CNN\")\n",
    "        print('-'*sum(len_assigned))\n",
    "\n",
    "        text = ''\n",
    "        for i in range(3):\n",
    "            text += col_names[i] + ' '*(len_assigned[i]-len(col_names[i]))\n",
    "        print(text)\n",
    "\n",
    "        print('='*sum(len_assigned))\n",
    "\n",
    "        total_params = 0\n",
    "        trainable_params = 0\n",
    "        non_trainable_params = 0\n",
    "\n",
    "        for i in range(len(self.layer_name)):\n",
    "            # layer name\n",
    "            layer_name = self.layer_name[i]\n",
    "            name = layer_name.lower() + '_' + str(count[layer_name]) + ' ' + '(' + layer_name + ')'\n",
    "            count[layer_name] += 1\n",
    "\n",
    "            # output shape\n",
    "            try:\n",
    "                out = '(None, ' \n",
    "                for n in range(len(model.architecture[i])-1):\n",
    "                    out += str(model.architecture[i][n]) + ', '\n",
    "                out += str(model.architecture[i][-1]) + ')'\n",
    "            except:\n",
    "                out = '(None, ' + str(self.architecture[i]) + ')'\n",
    "\n",
    "            # number of params\n",
    "            if layer_name=='Dense':\n",
    "                h0 = self.architecture[i-1]\n",
    "                h1 = self.architecture[i]\n",
    "                if self.layers[i-1].use_bias:\n",
    "                    params = h0*h1 + h1\n",
    "                else:\n",
    "                    params = h0*h1\n",
    "                total_params += params\n",
    "                trainable_params += params\n",
    "            elif layer_name=='BatchNormalization':\n",
    "                h = self.architecture[i]\n",
    "                params = 4*h\n",
    "                trainable_params += 2*h\n",
    "                non_trainable_params += 2*h\n",
    "                total_params += params\n",
    "            elif layer_name=='Conv2D':\n",
    "                layer = self.layers[i-1]\n",
    "                if layer.use_bias:\n",
    "                    add_b = 1\n",
    "                else:\n",
    "                    add_b = 0\n",
    "                params = ((layer.Nc * layer.Kh * layer.Kw) + add_b) * layer.F\n",
    "                trainable_params += params\n",
    "                total_params += params\n",
    "            else:\n",
    "                # Pooling, Dropout, Flatten, Input\n",
    "                params = 0\n",
    "            names = [name, out, str(params)]\n",
    "\n",
    "            # print this row\n",
    "            text = ''\n",
    "            for j in range(3):\n",
    "                text += names[j] + ' '*(len_assigned[j]-len(names[j]))\n",
    "            print(text)\n",
    "            if i!=(len(self.layer_name)-1):\n",
    "                print('-'*sum(len_assigned))\n",
    "            else:\n",
    "                print('='*sum(len_assigned))\n",
    "\n",
    "        print(\"Total params:\", total_params)\n",
    "        print(\"Trainable params:\", trainable_params)\n",
    "        print(\"Non-trainable params:\", non_trainable_params)\n",
    "        print('-'*sum(len_assigned))\n",
    "\n",
    "    def compile(self, cost_type, optimizer_type):\n",
    "        self.cost = Cost(cost_type)\n",
    "        self.cost_type = cost_type\n",
    "        self.optimizer_type = optimizer_type\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        if self.network_architecture_called==False:\n",
    "            self.network_architecture()\n",
    "            self.network_architecture_called = True\n",
    "        # initialize parameters for different layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if layer.__class__.__name__ in ['Dense', 'Conv2D']:\n",
    "                layer.initialize_parameters(self.architecture[i], self.optimizer_type)\n",
    "            elif layer.__class__.__name__=='BatchNormalization':\n",
    "                layer.initialize_parameters(self.architecture[i])\n",
    "\n",
    "    def fit(self, X, y, epochs=10, batch_size=5, lr=1, X_val=None, y_val=None, verbose=1, lr_decay=None, **kwargs):\n",
    "        \n",
    "        self.history = {'Training Loss': [],\n",
    "                        'Validation Loss': [],\n",
    "                        'Training Accuracy': [],\n",
    "                        'Validation Accuracy': []}\n",
    "\n",
    "        iterations = 0\n",
    "        self.m = batch_size\n",
    "        self.initialize_parameters()\n",
    "\n",
    "        total_num_batches = np.ceil(len(X)/batch_size)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            cost_train = 0\n",
    "            num_batches = 0\n",
    "            y_pred_train = []\n",
    "            y_train = []\n",
    "            \n",
    "            print('\\nEpoch: ' + str(epoch+1) + '/' + str(epochs))\n",
    "\n",
    "            for i in tqdm(range(0, len(X), batch_size)):\n",
    "                X_batch = X[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                \n",
    "                Z = X_batch.copy()\n",
    "\n",
    "                # feed-forward\n",
    "                for layer in self.layers:\n",
    "                    Z = layer.forward(Z)\n",
    "\n",
    "                # calculating training accuracy\n",
    "                if self.cost_type=='cross-entropy':\n",
    "                    y_pred_train += np.argmax(Z, axis=1).tolist()\n",
    "                    y_train += np.argmax(y_batch, axis=1).tolist()\n",
    "                \n",
    "                # calculating the loss\n",
    "                cost_train += self.cost.get_cost(Z, y_batch) / self.m\n",
    "\n",
    "                # calculating dL/daL (last layer backprop error)\n",
    "                dZ = self.cost.get_d_cost(Z, y_batch)\n",
    "                \n",
    "                # backpropagation\n",
    "                for layer in self.layers[::-1]:\n",
    "                    dZ = layer.backpropagation(dZ)\n",
    "\n",
    "                # Parameters update\n",
    "                for layer in self.layers:\n",
    "                    if layer.__class__.__name__ in ['Dense', 'BatchNormalization', 'Conv2D']:\n",
    "                        layer.update(lr, self.m, iterations)\n",
    "\n",
    "                # Learning rate decay\n",
    "                if lr_decay is not None:\n",
    "                    lr = lr_decay(iterations, **kwargs)\n",
    "\n",
    "                num_batches += 1\n",
    "                iterations += 1\n",
    "            \n",
    "            cost_train /= num_batches\n",
    "\n",
    "            # printing purpose only (Training Accuracy, Validation loss and accuracy)\n",
    "\n",
    "            text  = 'Training Loss: ' + str(round(cost_train, 4)) + ' - '\n",
    "            self.history['Training Loss'].append(cost_train)\n",
    "\n",
    "            # training accuracy\n",
    "\n",
    "            if self.cost_type=='cross-entropy':\n",
    "                accuracy_train = np.sum(np.array(y_pred_train) == np.array(y_train)) / len(y_train)\n",
    "                text += 'Training Accuracy: ' + str(round(accuracy_train, 4))\n",
    "                self.history['Training Accuracy'].append(accuracy_train)\n",
    "            else:\n",
    "                text += 'Training Accuracy: ' + str(round(cost_train, 4))\n",
    "                self.history['Training Accuracy'].append(cost_train)\n",
    "            \n",
    "            if X_val is not None:\n",
    "                cost_val, accuracy_val = self.evaluate(X_val, y_val, batch_size)\n",
    "                text += ' - Validation Loss: ' + str(round(cost_val, 4)) + ' - '\n",
    "                self.history['Validation Loss'].append(cost_val)\n",
    "                text += 'Validation Accuracy: ' + str(round(accuracy_val, 4))\n",
    "                self.history['Validation Accuracy'].append(accuracy_val)\n",
    "\n",
    "            if verbose:\n",
    "                print(text)\n",
    "            else:\n",
    "                print()\n",
    "\n",
    "    def evaluate(self, X, y, batch_size=None):\n",
    "        \n",
    "        if batch_size is None:\n",
    "            batch_size = len(X)\n",
    "\n",
    "        cost = 0\n",
    "        correct = 0\n",
    "        num_batches = 0\n",
    "        utility = Utility()\n",
    "        Y_1hot, _ = utility.onehot(y)\n",
    "\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            X_batch = X[i:i+batch_size]\n",
    "            y_batch = y[i:i+batch_size]\n",
    "            Y_1hot_batch = Y_1hot[i:i+batch_size]\n",
    "            Z = X_batch.copy()\n",
    "            for layer in self.layers:\n",
    "                if layer.__class__.__name__=='BatchNormalization':\n",
    "                    Z = layer.forward(Z, mode='test')\n",
    "                else:\n",
    "                    Z = layer.forward(Z)\n",
    "            if self.cost_type=='cross-entropy':\n",
    "                cost += self.cost.get_cost(Z, Y_1hot_batch) / len(y_batch)\n",
    "                y_pred = np.argmax(Z, axis=1).tolist()\n",
    "                correct += np.sum(y_pred == y_batch)\n",
    "            else:\n",
    "                cost += self.cost.get_cost(Z, y_batch) / len(y_batch)\n",
    "\n",
    "            num_batches += 1\n",
    "\n",
    "        if self.cost_type=='cross-entropy':\n",
    "            accuracy = correct / len(y)\n",
    "            cost /= num_batches\n",
    "            return cost, accuracy\n",
    "        else:\n",
    "            cost /= num_batches\n",
    "            return cost, cost\n",
    "\n",
    "    def loss_plot(self):\n",
    "        plt.plot(self.history['Training Loss'], 'k')\n",
    "        if len(self.history['Validation Loss'])>0:\n",
    "            plt.plot(self.history['Validation Loss'], 'r')\n",
    "            plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "            plt.title('Model Loss')\n",
    "        else:\n",
    "            plt.title('Training Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.show()\n",
    "\n",
    "    def accuracy_plot(self):\n",
    "        plt.plot(self.history['Training Accuracy'], 'k')\n",
    "        if len(self.history['Validation Accuracy'])>0:\n",
    "            plt.plot(self.history['Validation Accuracy'], 'r')\n",
    "            plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "            plt.title('Model Accuracy')\n",
    "        else:\n",
    "            plt.title('Training Accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X, batch_size=None):\n",
    "        \n",
    "        if batch_size==None:\n",
    "            batch_size = len(X)\n",
    "\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            X_batch = X[i:i+batch_size]\n",
    "            Z = X_batch.copy()\n",
    "            for layer in self.layers:\n",
    "                if layer.__class__.__name__=='BatchNormalization':\n",
    "                    Z = layer.forward(Z, mode='test')\n",
    "                else:\n",
    "                    Z = layer.forward(Z)\n",
    "            if i==0:\n",
    "                if self.cost_type=='cross-entropy':\n",
    "                    y_pred = np.argmax(Z, axis=1).tolist()\n",
    "                else:\n",
    "                    y_pred = Z\n",
    "            else:\n",
    "                if self.cost_type=='cross-entropy':\n",
    "                    y_pred += np.argmax(Z, axis=1).tolist()\n",
    "                else:\n",
    "                    y_pred = np.vstack((y_pred, Z))\n",
    "        \n",
    "        return np.array(y_pred)\n",
    "    def save_weights(self, file_path):\n",
    "        weights_to_save = [layer.get_weights() for layer in self.layers]\n",
    "        np.save(file_path, weights_to_save)\n",
    "\n",
    "    def load_weights(self, file_path):\n",
    "        loaded_weights = np.load(file_path, allow_pickle=True)\n",
    "        for layer, weights in zip(self.layers, loaded_weights):\n",
    "            layer.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c570f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data(dataset_path, image_size):\n",
    "    classes= os.listdir(dataset_path)\n",
    "    x_data=[]\n",
    "    y_data=[]\n",
    "    \n",
    "    for class_index,class_name in enumerate(classes):\n",
    "        class_path= os.path.join(dataset_path, class_name)\n",
    "        images= os.listdir(class_path)\n",
    "        \n",
    "        for image_name in images:\n",
    "            image_path= os.path.join(class_path,image_name)\n",
    "            image= cv2.imread(image_path,cv2.IMREAD_GRAYSCALE)\n",
    "            image= cv2.resize(image,(image_size,image_size))\n",
    "            x_data.append(image)\n",
    "            y_data.append(class_index)\n",
    "            \n",
    "    x_data=np.array(x_data)\n",
    "    y_data=np.array(y_data)\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "836a4a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path= \"C:\\\\Users\\\\91869\\\\Desktop\\\\cnn\\\\corn\\\\\"\n",
    "image_size= 32\n",
    "\n",
    "x,y= load_data(dataset_path,image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e04e1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 32, 32), 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.1,random_state=123)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1], X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1], X_test.shape[2])\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "utility = Utility()\n",
    "\n",
    "\n",
    "# train validation split\n",
    "X_train_new, X_val, y_train_new, y_val = utility.train_test_split(X_train, y_train, test_ratio=0.2, seed=42)\n",
    "\n",
    "Y_1hot_train, _ = utility.onehot(y_train_new)\n",
    "\n",
    "input_shape = X_train_new.shape[1:]\n",
    "output_dim = Y_1hot_train.shape[1]\n",
    "\n",
    "input_shape, output_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e459c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "\n",
    "model.add(model.Input(input_shape=input_shape))\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(5, 5), p='same', activation_type=\"relu\"))\n",
    "\n",
    "model.add(Pooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), p='same', activation_type=\"relu\"))\n",
    "\n",
    "model.add(Pooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(output_dim, activation_type=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73cf137d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: CNN\n",
      "--------------------------------------------------------------------------------------\n",
      "Layer (type)                                 Output Shape              # of Parameters\n",
      "======================================================================================\n",
      "input_1 (Input)                              (None, 1, 32, 32)         0              \n",
      "--------------------------------------------------------------------------------------\n",
      "conv2d_1 (Conv2D)                            (None, 16, 32, 32)        416            \n",
      "--------------------------------------------------------------------------------------\n",
      "pooling2d_1 (Pooling2D)                      (None, 16, 16, 16)        0              \n",
      "--------------------------------------------------------------------------------------\n",
      "conv2d_2 (Conv2D)                            (None, 32, 16, 16)        4640           \n",
      "--------------------------------------------------------------------------------------\n",
      "pooling2d_2 (Pooling2D)                      (None, 32, 8, 8)          0              \n",
      "--------------------------------------------------------------------------------------\n",
      "flatten_1 (Flatten)                          (None, 2048)              0              \n",
      "--------------------------------------------------------------------------------------\n",
      "dropout_1 (Dropout)                          (None, 2048)              0              \n",
      "--------------------------------------------------------------------------------------\n",
      "dense_1 (Dense)                              (None, 4)                 8196           \n",
      "======================================================================================\n",
      "Total params: 13252\n",
      "Trainable params: 13252\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c09f52a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 12\n",
    "lr = 0.05\n",
    "\n",
    "model.compile(cost_type=\"cross-entropy\", optimizer_type=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0457dc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [27:20<00:00, 149.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 15.3723 - Training Accuracy: 0.2987 - Validation Loss: 16.1287 - Validation Accuracy: 0.3213\n",
      "\n",
      "Epoch: 2/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [26:23<00:00, 143.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 15.7841 - Training Accuracy: 0.3146 - Validation Loss: 15.7875 - Validation Accuracy: 0.3357\n",
      "\n",
      "Epoch: 3/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [25:52<00:00, 141.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 14.6793 - Training Accuracy: 0.3525 - Validation Loss: 14.4571 - Validation Accuracy: 0.3804\n",
      "\n",
      "Epoch: 4/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [27:10<00:00, 148.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 13.4877 - Training Accuracy: 0.3943 - Validation Loss: 12.1903 - Validation Accuracy: 0.4409\n",
      "\n",
      "Epoch: 5/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [26:06<00:00, 142.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 12.0848 - Training Accuracy: 0.4376 - Validation Loss: 12.281 - Validation Accuracy: 0.4424\n",
      "\n",
      "Epoch: 6/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [09:58<00:00, 54.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 11.4568 - Training Accuracy: 0.4603 - Validation Loss: 11.3287 - Validation Accuracy: 0.4539\n",
      "\n",
      "Epoch: 7/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [09:57<00:00, 54.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 11.0524 - Training Accuracy: 0.4614 - Validation Loss: 10.5779 - Validation Accuracy: 0.4813\n",
      "\n",
      "Epoch: 8/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [09:59<00:00, 54.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 9.7727 - Training Accuracy: 0.4614 - Validation Loss: 9.0722 - Validation Accuracy: 0.4841\n",
      "\n",
      "Epoch: 9/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [10:01<00:00, 54.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 7.4966 - Training Accuracy: 0.4812 - Validation Loss: 6.5242 - Validation Accuracy: 0.451\n",
      "\n",
      "Epoch: 10/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                          | 1/11 [00:54<09:05, 54.57s/it]"
     ]
    }
   ],
   "source": [
    "LR_decay = LearningRateDecay()\n",
    "\n",
    "model.fit(X_train_new, Y_1hot_train, epochs=epochs, batch_size=batch_size, lr=lr, X_val=X_val, \n",
    "          y_val=y_val, verbose=1, lr_decay=LR_decay.constant, lr_0=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb3446d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
